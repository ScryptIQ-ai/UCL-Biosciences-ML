{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Your Dataset for Machine Learning\n",
    "\n",
    "With all analysis, machine learning or not, it's best to start by getting to grips with your dataset. In this notebook we will demonstrate how to use Python to visualise and characterise the dataset.\n",
    "\n",
    "## Key objectives\n",
    "- Load, explore and visualise the dataset\n",
    "- Apply basic quality criteria, like checks for missing values\n",
    "- Check for class imbalance\n",
    "- Calculate correlations between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducing the Indian Liver Patient Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information from Kaggle\n",
    "\n",
    "This dataset contains 416 liver patient records and 167 non-liver-patient records.The dataset was collected from test samples in North East of Andhra Pradesh, India. \n",
    "\n",
    "'is_patient' is a binary class label used to divide patients into two groups: liver patient or not. This dataset contains:\n",
    "\n",
    "- 441 male patient records \n",
    "- 142 female patient records.\n",
    "\n",
    "**Note**: Any patient whose age exceeded 89 is listed as being of age \"90\".\n",
    "\n",
    "[Indian Liver Patient Dataset](https://www.kaggle.com/datasets/jeevannagaraj/indian-liver-patient-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Our Environment\n",
    "\n",
    "Now that we understand the origins of our dataset, let's explore it. \n",
    "\n",
    "For this we will be using a combination of widely-used data science packages in Python. Here is a brief description of each:\n",
    "\n",
    "- **Pandas**: A powerful data manipulation library that provides its own DataFrame structures ideal for working with labelled, tabular data\n",
    "\n",
    "- **NumPy**: A fundamental package for numerical computing in Python, providing support for arrays and a plethora of mathematical functions\n",
    "\n",
    "- **Matplotlib**: A comprehensive plotting library capable of creating static, animated and interactive visualisations\n",
    "\n",
    "- **Seaborn**: A statistical data visualisation library that builds on matplotlib and provides a high-level interface for drawing attractive statistical graphs\n",
    "\n",
    "- **scikit-learn**: A popular Python machine learning library that provides simple and efficient tools for data analysis and modelling\n",
    "\n",
    "Using a combination of these tools, we should be able to load, manipulate and visualise our data powerfully, and effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our packages, giving some shorter aliases to make typing easier\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "\n",
    "# Set visualisation style for consistency\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is to check that the data we have are as we expect. We can do the following:\n",
    "\n",
    "- Check how many rows (**samples**) and columns (**features**) we have\n",
    "- Take a sneak-peak at the actual data\n",
    "- See what types we have in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/indian_liver_patient.csv\") # Read in the CSV file as a Pandas DataFrame\n",
    "\n",
    "# Display basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first five rows to understand the structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of all columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the DataFrame is all numeric, with features that are *floats*, and a binary diagnosis column with values of either 0 or 1, which represent our labels (the outcome of a diagnosis, and what we are hoping to predict in this practical). \n",
    "\n",
    "We can confirm if this is all true, by counting the values of the DataFrame's `dtypes`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for how many unique data types are present in the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Data Quality Checks\n",
    "\n",
    "Data quality is crucial. In this example, we are lucky enough to be using a well-prepared, clean dataset; but with real-world problems, this is often not the case! In this section we will run through some common data quality checks. \n",
    "\n",
    "**Null** or **missing** values will often break machine learning models, and so we need to appropriately handle them before we begin training. There are a few common strategies for **handling null values**. You can: \n",
    "\n",
    "- Remove the offending row altogether\n",
    "- Use a replacement value (such as zero, or the average of the other values in the column)\n",
    "- Interpolate the value based upon the other features in the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "title = \"Distribution of classes:\"\n",
    "title_length = len(title)\n",
    "\n",
    "print(title, \"=\" * title_length, sep=\"\\n\")\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "\n",
    "df = df[~df['alkphos'].isnull()] # ~ here is a NOT operator, meaning that all values that do not satisfy this expression, are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values\n",
    "\n",
    "df['alkphos'] = df['alkphos'].fillna(np.mean(df['alkphos'])) # This replaces all missing values in this column with the mean of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also good practice to check for duplicates. These occur more often than you might think. Leaving duplicates in yoru data can introduce bias when training your machine learning model, in favour of the duplicated data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any of our rows were duplicated, we could filter them out like this\n",
    "df = df[~df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b - Augmenting labeled data\n",
    "\n",
    "Text labels need to be converted to a numerical form to be read and analysed by machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what our gender distribution looks like:\n",
    "\n",
    "df[['gender', 'is_patient']].groupby('gender').agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then reset the indexing column, as we performed operations earlier to remove missing values. When you remove these data values, the Pandas DataFrame retains the original indexing, and so you may get indexing that labels your rows in broken sequence. Using the `.reset_index` method, we can both drop (remove) the old column of indexes, reset the indexing to run concurrently with our modified DataFrame, and make this modification directly to our DataFrame.\n",
    "\n",
    "*HINT: Whenever you see the keyword argument* `inplace=True`, *this means that it is performing the operation on the original DataFrame, and modifying it, directly (as opposed to creating a copy).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As aforementioned, machine learning models do not like to work with text labels. They understand numerical values, only, and so we must account for this, by converting text labels (in this case, 'gender' and 'is_patient') to numerical values that the model will understand.\n",
    "\n",
    "To do this we can use what's known as **one-hot encoding**. If we use the gender example, this will take a column from being a single column containig data entries 'Male' and 'Female', and convert it into two columns: one for Male and another for Female, with a binary 0 or 1 to represent whether the respective gender is present in the entry, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode our data\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist() # Identify categorical features (with text labels).\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False) # Instantiate the OneHotEncoder model. The keyword argument specifies the output is a NumPy array.\n",
    "\n",
    "one_hot_encoded = encoder.fit_transform(df[categorical_columns]) # Learn categories, and convert them into one-hot-encoded arrays.\n",
    "\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns)).astype(int) # Convert these arrays into a DataFrame.\n",
    "\n",
    "df_encoded = df.join(one_hot_df) # Add these new DataFrame columns to the existing DataFrame\n",
    "\n",
    "df_encoded = df_encoded.drop(categorical_columns, axis=1) # Drop (remove) the old categorical columns.\n",
    "\n",
    "print(f\"Encoded dataset : \\n{df_encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that `gender_Female` and `gender_Male` data is opposite of each other for obvious reasons. Due to this we can actually remove one of the columns as the negative of the other is informative enough, so there is not need to duplicate this data. \n",
    "\n",
    "This may seem long winded, but using the one hot encoding tool can be used on columns with many classes, so it is good to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.drop(columns = ['gender_Male']) # drop only gender_Male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: As a cautionary measure, we made our changes and stored them in a new DataFrame object called `df_encoded`. Now that we are happy with how the DataFrame looks, we can overwrite the original `df`, and replace its contents with those of `df_encoded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've modified \n",
    "\n",
    "df = df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Variable Analysis\n",
    "\n",
    "The goal of this kind of machine learning is to predict one aspect of a data point, based on the others. The thing you're trying to predict is referred to as the **label**, the **class**, or the **target variable**. In this case, the label is the *diagnosis*, with two possible values: 1 (not a liver patient) and 2 (is a liver patient).\n",
    "\n",
    "**Class imbalance** can significantly affect model performance, so understanding the class distribution is crucial for machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and simple way to print the distribution of our target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are following the more recognised convention of categorical data being given a 0 for a feature being absent or `False`, and 1 for a feature being present or `True`, let's correct this dataset's values of 1 and 2, to align with this using the `.replace()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the distribution of the target variable\n",
    "class_dist = df['is_patient'].value_counts() \n",
    "title = \"Distribution of classes:\"\n",
    "title_length = len(title)\n",
    "\n",
    "print(title, \"=\" * title_length, sep=\"\\n\") # Underlines the title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: On the subject of **class imbalance**, an unbalanced distribution of classes in the target variable can affect your predictions with machine learning. If one class dominates, the algorithm might achieve high accuracy by simply predicting the majority class. In this dataset, there is a fairly large skew towards the negative (non-patient) class: something to keep in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful way to get an overview of the features is to look at the summary statistics - the mean, standard deviation, and quartile values - for each column. We can do that easily with Pandas. \n",
    "\n",
    "Let's firstly generate a quick preview of the DataFrame again, using the `.head()` method. By default, this displays the first five samples (rows), and all features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this statistical summary, let's again create a new DataFrame called `df_features`, and have it contain only the features (removing the categorical variables). Categorical values - even though we one-hot-encoded these to give meaningful numerical values - aren't actually meaningful beyond being binary labels, and would thus be uninformative to perform statistical analyses upon.\n",
    "\n",
    "Thus, our new DataFrame will:\n",
    "\n",
    "- Exclude the target variable (the label we want to later predict using our trained machine learning model) \n",
    "- Exclude gender, for which summary statistics would be largely redundant\n",
    "\n",
    "As the target variable is also the outcome we want to predict, removing it at this stage, is good exploratory practice. Namely because it isn't truly a feature, but the binary label we want to predict. We can then use the `.describe()` method to get a statistical summary of the remaining data in our new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame containing only the features (excluding the target variable)\n",
    "\n",
    "# Get a statistical summary of numerical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Distributions\n",
    "\n",
    "Understanding how features are distributed is essential for choosing appropriate preprocessing techniques and algorithms. Machine learning algorithms generally look for ways to separate points of different classes by finding high-dimensional patterns. These patterns are often very difficult for us to visualise, but what we can do is to break down the problem, and look at a couple of dimensions at a time. \n",
    "\n",
    "The **pairplot** from Seaborn is a great starting tool, to eyeball the data, and at a glance, determine if there are any pairs of features that show clear differences between the classes. \n",
    "\n",
    "If you see multiple pairs with *decent visual separation between classes*, there is a *good chance a machine learning model will be perform well*. Many pairs might have slight separation with a lot of 'blur' between the classes; however, in a higher-dimensional space the boundary will hopefully be more defined.\n",
    "\n",
    "The figure will be quite large, as this dataset has a lot of features, and so we need to reduce the size and resolution a little to make it display nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df.drop(columns=['gender_Female']), # Drop gender column\n",
    "                 hue='is_patient', palette={0: 'green', 1: 'red'},  # Have the colouring correspond to diagnosis\n",
    "                 height=1.2, plot_kws={'alpha':0.6}) # Specify height, and set 60% transparency between classes\n",
    "                                                     # so that overlap is easily visible.\n",
    "\n",
    "g.figure.set_dpi(60) # Set the resolution of the figure to 60 dots per inch.\n",
    "\n",
    "plt.show() # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Scaling and Comparison\n",
    "\n",
    "Features often have different scales. Look at the summary statistics above; `alkphos` ranges from 0.3 to 2.80, whereas `tot_proteins` goes from 63 to 2110. \n",
    "\n",
    "Some machine learning algorithms find it *difficult* to compare features that range over such different magnitudes. Features whose units are larger can swamp the predictive space and have a disproportionately, greater affect the predictions made by the algorithm. \n",
    "\n",
    "To combat this, we *rescale* the data so that all the features vary over the same range. In this example, we can use the `StandardScaler()` class imported from `scikit-learn` in order to do this. \n",
    "\n",
    "**Note**: There are other [scalers](https://scikit-learn.org/stable/modules/preprocessing.html#), each with their own advantages and disadvantages; you can learn more about them via scikit-learn's documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature means before scaling\n",
    "feature_means = df_features.mean()\n",
    "\n",
    "# Make a single figure that will hold and display two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6), dpi=80)\n",
    "\n",
    "# Original scale\n",
    "ax1.bar(range(len(feature_means)), feature_means.values)\n",
    "ax1.set_xlabel('Feature Index')\n",
    "ax1.set_ylabel('Mean Value')\n",
    "ax1.set_title('Feature Means - Original Scale')\n",
    "ax1.xaxis.set_ticks(np.arange(len(df_features.columns)))\n",
    "ax1.set_xticklabels(df_features.columns, rotation=45, ha='right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Apply robust scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_features)\n",
    "scaled_means = pd.DataFrame(scaled_features).describe().loc['mean']\n",
    "\n",
    "# Scaled features\n",
    "ax2.bar(df_features.columns, scaled_means)\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.set_ylabel('Mean Value (Scaled)')\n",
    "ax2.set_title('Feature Means - After Standard Scaling')\n",
    "ax2.xaxis.set_ticks(np.arange(len(df_features.columns)))\n",
    "ax2.set_xticklabels(df_features.columns, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HINT: To truly see the differences between scaled and unscaled features, look at the values on the y-axes of each plot.*\n",
    "\n",
    "Once we are happy with our scaled data, we can overwrite the original DataFrame values with the newly-scaled features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our new scaled data back to the DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get a snapshot of our summary statistics now, using the `.describe()` method. Note, we've been lazy, and haven't removed the target variable or gender, here - ignore them. In fact, leaving them in explains why we removed them, above (having a patient who is 0.42% male isn't meaningful information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a glance at our DataFrame, now that we have done our data exploration and pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with the data, and are ready to move onto our machine learning analyses, let's save the pre-processed DataFrame to a CSV file, using Pandas' `.to_csv()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as CSV file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we'll start to perform some modelling using classical machine learning methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
