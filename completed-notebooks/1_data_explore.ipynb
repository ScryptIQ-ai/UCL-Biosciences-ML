{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Your Dataset for Machine Learning\n",
    "\n",
    "With all analysis, machine learning or not, it's best to start by getting to grips with your dataset. In this notebook we will demonstrate how to use Python to visualise and characterise the dataset.\n",
    "\n",
    "## Key objectives\n",
    "- Load, explore and visualise the dataset\n",
    "- Apply basic quality criteria, like checks for missing values\n",
    "- Check for class imbalance\n",
    "- Calculate correlations between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducing the Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this series is the Wisconsin Breast Cancer Diagnostic Database, publicly available from the [University of California Irvine (UCI) Machine Learning Repository](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). It consists of characteristics, or features (the machine learning term), of cell nuclei taken from breast masses which were sampled using fine-needle aspiration (FNA), a common diagnostic procedure in oncology. \n",
    "\n",
    "The clinical samples used to form this dataset were collected from January 1989 to November 1991. Relevant features were extracted from the digital images using Multisurface Method-Tree (K.P. Bennett 1992). This is a classification method that uses linear programming to construct a decision tree - a form of machine learning! \n",
    "\n",
    "The features are observable characteristics of the cells in the images - for example, radius, concavity, and texture. An example of one of the digitised images from an FNA sample is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![An example of an image of a breast mass from which dataset features were extracted](../assets/fna_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Knowing your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it important to understand the origins of your dataset? The data we're working with here was collected around 1990. While the data points (features) collected in a new sample may theoretically be the same, the hardware used to collect them and the processing methods may differ. To the eye, they could look identical, but there might be subtle changes in the patterns within the data. A model learns the patterns and properties present in whatever data it is trained on, not necessarily the \"true\" characteristics of breast cancer cells. When asked to predict on cancer cells collected in 2025, it might give us incorrect answers, and if we weren't careful we might mistakenly trust these answers, given the model's strong performance on historical data. This phenomenon is called \"dataset shift\" or \"distribution shift,\" and it's something you should always keep in mind when using someone else's data, or even your own. Reproducibility is key!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Our Environment\n",
    "\n",
    "Now we understand the origins of our dataset, let's explore it. \n",
    "\n",
    "For this we will be using a combination of widely-used data science packages in Python. Here is a brief description of each:\n",
    "\n",
    "- **Pandas**: A powerful data manipulation library that provides DataFrame structures ideal for working with labelled, tabular data\n",
    "- **Numpy**: The fundamental package for numerical computing in Python, providing support for arrays and mathematical functions\n",
    "- **Matplotlib**: A comprehensive plotting library capable of creating static, animated, and interactive visualisations\n",
    "- **Seaborn**: A statistical data visualisation library that builds on matplotlib and provides a high-level interface for drawing attractive statistical graphs\n",
    "- **Scikit-learn**: A machine learning library that provides simple and efficient tools for data analysis and modelling\n",
    "\n",
    "Using a combination of these tools, we should be able to load, manipulate, and visualise our data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our packages, giving some shorter aliases to make typing easier\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set visualisation style for consistency\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is to check that the data we have is as we expected. We can check how many rows (samples) and columns (features) we have, take a sneak-peak at the actual data, and see what types we have in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/breast_cancer.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of samples: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows to understand the structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of all columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the dataframe is all numeric, with `float64` features and an integer column for the diagnosis (the labels). We can check this by counting the values of the DataFrame's `dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for how many unique data types are present in the dataframe\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Checks\n",
    "\n",
    "Data quality is crucial. We're lucky enough to be using a well-prepared, clean dataset, but in real-world problems that's not often the case! In this section we'll run through some common data quality checks. \n",
    "\n",
    "Null or missing values will often break machine learning models, so we need to appropriately handle them before we being training. There are a few common strategies for handling null values. You can remove the offending row altogether; use a replacement value (such as zero, or the average of the other values in the column); or interpolate the value based upon the other features in the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(\"-\" * 50)\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also good to check for duplicates. This is more commons than you might think. Duplicates will introduce bias in training, slightly favouring the duplicated data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any of our rows were duplicated, we could filter them like this\n",
    "df_deduped = df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Variable Analysis\n",
    "\n",
    "The goal of this kind of machine learning is to predict one aspect of a data point, based on the others. The thing you're trying to predict is called the label, the class, or the target variable. In this case, the label is the diagnosis, with two possible values: 0 (benign) and 1 (malignant).\n",
    "\n",
    "Class imbalance can significantly affect model performance, so understanding the class distribution is crucial for machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and simple way to print the distribution of our target variable\n",
    "df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class mapping for better readability\n",
    "cls_dict = {0: 'benign', 1: 'malignant'}\n",
    "\n",
    "# Analyse the distribution of the target variable\n",
    "class_dist = df['diagnosis'].value_counts()\n",
    "\n",
    "print(\"Distribution of classes:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Benign samples: {class_dist[0]} ({class_dist[0] / len(df) * 100:.1f}%)\")\n",
    "print(f\"Malignant samples: {class_dist[1]} ({class_dist[1] / len(df) * 100:.1f}%)\")\n",
    "print(f\"\\nClass ratio (benign:malignant): {class_dist[0]/class_dist[1]:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on Class Imbalance**: An unbalanced distribution of classes in the target variable can affect your predictions with machine learning. If one class dominates, the algorithm might achieve high accuracy by simply predicting the majority class. In this dataset, we have a reasonable balance (approximately 1.7:1 ratio), though it's worth noting that the malignant class is slightly more prevalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful way to get an overview of the features is to look at the summary statistics - the mean, standard deviation, and quartile values - for each column. We can do that easily with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe containing only the features (excluding the target variable)\n",
    "df_features = df.drop(columns=['diagnosis'])\n",
    "\n",
    "# Get statistical summary of numerical features\n",
    "df_features.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Distributions\n",
    "\n",
    "Understanding how features are distributed is essential for choosing appropriate preprocessing techniques and algorithms. Machine learning algorithms generally look for ways to separate points of different classes by finding high-dimensional patterns. These patterns are very difficult for our puny human brains to visualise, but what we can do is break down the problem and look at a couple of dimensions at a time. \n",
    "\n",
    "The pairplot from seaborn is a great starting tool, to see roughly at a glance if there are any pairs of features that show clear differences between the classes. If you can see multiple pairs with visual separation between classes, there is a good chance a machine learning model will be perform well. Many pairs might have slight separation with a lot of 'blur' between the classes; however, in a higher dimensional space the boundry will hopefully be more defined.\n",
    "\n",
    "The figure comes out qutie large because we have a lot of features, so we need to reduce the size and resolution a little to make it display nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df, hue='diagnosis', palette={0: 'green', 1: 'red'}, height=1.2, plot_kws={'alpha':0.6})\n",
    "g.figure.set_dpi(60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Scaling and Comparison\n",
    "\n",
    "Features often have different scales. Look at the summary statistics above; `smoothness_mean` ranges from 0.05 to 0.16, whereas `area` goes from 143.5 to 2501. Some machine learning algorithms find it difficult to compare features that range over such different magnitudes. Features whose units are larger can swamp the predictive space and have a disproportionately greater affect upon the predictions made by the algorithm. To combat this, we rescale the data so that all the features vary over the same range. We do this with the `RobustScaler()` class imported from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature means before scaling\n",
    "feature_means = df_features.mean()\n",
    "\n",
    "# Make a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6), dpi=80)\n",
    "\n",
    "# Original scale\n",
    "ax1.bar(range(len(feature_means)), feature_means.values)\n",
    "ax1.set_xlabel('Feature Index')\n",
    "ax1.set_ylabel('Mean Value')\n",
    "ax1.set_title('Feature Means - Original Scale')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Apply robust scaling\n",
    "scaler = RobustScaler()\n",
    "scaled_features = scaler.fit_transform(df_features)\n",
    "scaled_means = np.mean(scaled_features, axis=0)\n",
    "\n",
    "# Scaled features\n",
    "ax2.bar(df_features.columns, scaled_means)\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.set_ylabel('Mean Value (Scaled)')\n",
    "ax2.set_title('Feature Means - After Robust Scaling')\n",
    "ax2.xaxis.set_ticks(np.arange(len(df_features.columns)))\n",
    "ax2.set_xticklabels(df_features.columns, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we'll start to perform some modelling using classical machine learning methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2d-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
